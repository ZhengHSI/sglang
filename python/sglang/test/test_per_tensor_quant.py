#!/usr/bin/env python3
"""
æµ‹è¯• per-tensor INT8 é‡åŒ–ç®—å­
"""

import torch
import sys
import os

# æ·»åŠ  sglang è·¯å¾„
# sys.path.insert(0, "sglang/python")

def test_per_tensor_quant():
    """æµ‹è¯• per-tensor é‡åŒ–åŠŸèƒ½"""
    print("ðŸ§ª å¼€å§‹æµ‹è¯• per-tensor INT8 é‡åŒ–ç®—å­")
    
    try:
        from sglang.srt.layers.quantization.int8_kernel import per_tensor_quant_int8
        print("âœ… æˆåŠŸå¯¼å…¥ per_tensor_quant_int8 å‡½æ•°")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„çŽ¯å¢ƒä¸­è¿è¡Œ")
        return False
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•ï¼ˆTriton éœ€è¦ CUDAï¼‰")
        return False
    
    device = "cuda"
    print(f"ðŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    print("\nðŸ“‹ åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    test_shapes = [
        (128, 256),        # 2D tensor
        (64, 128, 256),    # 3D tensor
        (4, 64, 128, 256), # 4D tensor
        (1024,),           # 1D tensor
    ]
    
    for i, shape in enumerate(test_shapes):
        print(f"  æµ‹è¯• {i+1}: å½¢çŠ¶ {shape}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(shape, device=device, dtype=torch.float16)
        
        try:
            # æµ‹è¯•ä¸å¸¦sumçš„é‡åŒ–
            x_q, scale = per_tensor_quant_int8(x, cal_sum=False)
            
            # éªŒè¯åŸºæœ¬å±žæ€§
            assert x_q.shape == x.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {x_q.shape} vs {x.shape}"
            assert x_q.dtype == torch.int8, f"è¾“å‡ºç±»åž‹é”™è¯¯: {x_q.dtype}"
            assert scale.shape == torch.Size([1]), f"Scaleå½¢çŠ¶é”™è¯¯: {scale.shape}"
            assert x_q.min() >= -128 and x_q.max() <= 127, "é‡åŒ–å€¼è¶…å‡ºint8èŒƒå›´"
            
            print(f"    âœ… é‡åŒ–èŒƒå›´: [{x_q.min().item()}, {x_q.max().item()}], Scale: {scale.item():.6f}")
            
            # æµ‹è¯•å¸¦sumçš„é‡åŒ–
            x_q_sum, scale_sum, x_sum = per_tensor_quant_int8(x, cal_sum=True)
            
            # éªŒè¯ä¸€è‡´æ€§
            assert torch.allclose(x_q, x_q_sum), "å¸¦/ä¸å¸¦sumçš„é‡åŒ–ç»“æžœä¸ä¸€è‡´"
            assert torch.allclose(scale, scale_sum), "å¸¦/ä¸å¸¦sumçš„scaleä¸ä¸€è‡´"
            
            print(f"    âœ… Sumè®¡ç®—: {x_sum.item():.2f} (åŽŸå§‹: {x.sum().item():.2f})")
            
        except Exception as e:
            print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    # ç²¾åº¦éªŒè¯æµ‹è¯•
    print("\nðŸŽ¯ ç²¾åº¦éªŒè¯æµ‹è¯•")
    x = torch.randn(512, 512, device=device, dtype=torch.float16)
    x_q, scale = per_tensor_quant_int8(x)
    
    # åé‡åŒ–
    x_dequant = x_q.float() * scale
    
    # è®¡ç®—è¯¯å·®
    mse = torch.mean((x.float() - x_dequant) ** 2)
    mae = torch.mean(torch.abs(x.float() - x_dequant))
    
    print(f"  MSE: {mse.item():.6f}")
    print(f"  MAE: {mae.item():.6f}")
    print(f"  ç›¸å¯¹è¯¯å·®: {(mae / torch.mean(torch.abs(x.float()))).item():.4%}")
    
    # è¾¹ç•Œæƒ…å†µæµ‹è¯•
    print("\nðŸ” è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    
    # æµ‹è¯•å…¨é›¶å¼ é‡
    x_zeros = torch.zeros(100, 100, device=device, dtype=torch.float16)
    x_q_zeros, scale_zeros = per_tensor_quant_int8(x_zeros)
    print(f"  å…¨é›¶å¼ é‡: Scale = {scale_zeros.item():.8f}, é‡åŒ–å€¼å…¨ä¸º0: {torch.all(x_q_zeros == 0).item()}")
    
    # æµ‹è¯•å¾ˆå°çš„å€¼
    x_small = torch.full((100, 100), 1e-8, device=device, dtype=torch.float16)
    x_q_small, scale_small = per_tensor_quant_int8(x_small)
    print(f"  æžå°å€¼å¼ é‡: Scale = {scale_small.item():.8f}")
    
    # æµ‹è¯•å¾ˆå¤§çš„å€¼
    x_large = torch.full((100, 100), 1000.0, device=device, dtype=torch.float16)
    x_q_large, scale_large = per_tensor_quant_int8(x_large)
    print(f"  å¤§å€¼å¼ é‡: Scale = {scale_large.item():.6f}, Maxé‡åŒ–å€¼: {x_q_large.max().item()}")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nâš¡ æ€§èƒ½æµ‹è¯•")
    shape = (1024, 4096)
    x = torch.randn(shape, device=device, dtype=torch.float16)
    
    # é¢„çƒ­
    for _ in range(10):
        _ = per_tensor_quant_int8(x)
    
    # è®¡æ—¶
    torch.cuda.synchronize()
    import time
    start_time = time.time()
    
    num_runs = 100
    for _ in range(num_runs):
        _ = per_tensor_quant_int8(x)
    
    torch.cuda.synchronize()
    total_time = time.time() - start_time
    avg_time = total_time / num_runs * 1000  # ms
    
    elements = x.numel()
    throughput = elements * num_runs / total_time / 1e9  # GElements/s
    
    print(f"  å¹³å‡æ—¶é—´: {avg_time:.2f} ms")
    print(f"  åžåé‡: {throughput:.2f} GElements/s")
    
    print("\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Per-tensor é‡åŒ–ç®—å­å·¥ä½œæ­£å¸¸")
    return True

def compare_with_pytorch():
    """ä¸ŽPyTorchåŽŸç”Ÿå®žçŽ°å¯¹æ¯”"""
    print("\nðŸ“Š ä¸ŽPyTorchåŽŸç”Ÿå®žçŽ°å¯¹æ¯”")
    
    def pytorch_per_tensor_quant(x):
        """PyTorchåŽŸç”Ÿper-tensoré‡åŒ–å®žçŽ°"""
        absmax = torch.max(torch.abs(x))
        scale = absmax / 127.0
        x_q = torch.clamp(torch.round(x / scale), -128, 127).to(torch.int8)
        return x_q, scale
    
    device = "cuda"
    x = torch.randn(512, 512, device=device, dtype=torch.float16)
    
    # æˆ‘ä»¬çš„å®žçŽ°
    try:
        from sglang.srt.layers.quantization.int8_kernel import per_tensor_quant_int8
        x_q_ours, scale_ours = per_tensor_quant_int8(x)
    except:
        print("  âŒ æ— æ³•å¯¼å…¥æˆ‘ä»¬çš„å®žçŽ°")
        return
    
    # PyTorchå®žçŽ°
    x_q_pt, scale_pt = pytorch_per_tensor_quant(x)
    
    # æ¯”è¾ƒç»“æžœ
    scale_diff = torch.abs(scale_ours - scale_pt)
    quant_diff = torch.mean(torch.abs(x_q_ours.float() - x_q_pt.float()))
    fp_diff = torch.mean(torch.abs(x_q_ours.float()*scale_ours - x.float()))

    print(f"  Scaleå·®å¼‚: {scale_diff.item():.8f}")
    print(f"  é‡åŒ–å€¼å¹³å‡å·®å¼‚: {quant_diff.item():.6f}")
    print(f"  é‡åŒ–è¯¯å·®: {fp_diff.item():.6f}")
    # æ€§èƒ½æ¯”è¾ƒ
    import time
    
    # é¢„çƒ­
    for _ in range(10):
        _ = per_tensor_quant_int8(x)
        _ = pytorch_per_tensor_quant(x)
    
    # æˆ‘ä»¬çš„å®žçŽ°
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = per_tensor_quant_int8(x)
    torch.cuda.synchronize()
    our_time = time.time() - start
    
    # PyTorchå®žçŽ°
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = pytorch_per_tensor_quant(x)
    torch.cuda.synchronize()
    pt_time = time.time() - start
    
    speedup = pt_time / our_time
    print(f"  æˆ‘ä»¬çš„å®žçŽ°: {our_time*1000:.2f} ms")
    print(f"  PyTorchå®žçŽ°: {pt_time*1000:.2f} ms")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")

if __name__ == "__main__":
    success = test_per_tensor_quant()
    if success:
        compare_with_pytorch() 