#!/usr/bin/env python3
"""
æµ‹è¯• MOE ä¸­å„ä¸ªç®—å­çš„ç²¾åº¦è¯¯å·®
"""

import torch
import sys
import numpy as np
from typing import Tuple, Optional

# æ·»åŠ  sglang è·¯å¾„
sys.path.insert(0, "sglang/python")

def create_moe_test_data(
    batch_size: int = 128,
    seq_len: int = 256, 
    hidden_size: int = 4096,
    intermediate_size: int = 11008,
    num_experts: int = 8,
    topk: int = 2,
    device: str = "cuda"
) -> dict:
    """
    åˆ›å»º MOE æµ‹è¯•æ•°æ®
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦  
        hidden_size: éšè—å±‚å¤§å°
        intermediate_size: ä¸­é—´å±‚å¤§å°
        num_experts: ä¸“å®¶æ•°é‡
        topk: æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        device: è®¾å¤‡
    
    Returns:
        åŒ…å«æ‰€æœ‰æµ‹è¯•æ•°æ®çš„å­—å…¸
    """
    # print(f"ğŸ—ï¸ åˆ›å»ºMOEæµ‹è¯•æ•°æ®...")
    # print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {seq_len}")
    # print(f"   éšè—å±‚å¤§å°: {hidden_size}, ä¸­é—´å±‚å¤§å°: {intermediate_size}")
    # print(f"   ä¸“å®¶æ•°é‡: {num_experts}, TopK: {topk}")
    
    M = batch_size * seq_len
    K = hidden_size
    N = intermediate_size
    
    # 1. è¾“å…¥æ¿€æ´»
    a = torch.randn(M, K, device=device, dtype=torch.half)
    print(f"   è¾“å…¥å½¢çŠ¶: {a.shape}")
    
    # 2. é‡åŒ–æƒé‡ (æ¨¡æ‹Ÿint4æƒé‡ç”¨int8å­˜å‚¨)
    # W1: [num_experts, N*2, K//2] - gateå’Œupæƒé‡åˆå¹¶, int4æ‰“åŒ…
    w1_q = torch.randint(-128, 127, (num_experts, N * 2, K // 2), 
                        device=device, dtype=torch.int8)
    
    # W2: [num_experts, K, N//2] - downæƒé‡, int4æ‰“åŒ…
    w2_q = torch.randint(-128, 127, (num_experts, K, N // 2), 
                        device=device, dtype=torch.int8)
    
    print(f"   W1æƒé‡å½¢çŠ¶: {w1_q.shape}")
    print(f"   W2æƒé‡å½¢çŠ¶: {w2_q.shape}")
    
    # 3. æƒé‡scales (group-wise quantization)
    # W1 scale: [num_experts, K//512, N*8] 
    w1_scale = torch.rand(num_experts, K // 512, N * 8, 
                         device=device, dtype=torch.bfloat16) * 0.1
    
    # W2 scale: [num_experts, N//512, K*4]
    w2_scale = torch.rand(num_experts, N // 512, K * 4, 
                         device=device, dtype=torch.bfloat16) * 0.1
    
    print(f"   W1 scaleå½¢çŠ¶: {w1_scale.shape}")
    print(f"   W2 scaleå½¢çŠ¶: {w2_scale.shape}")
    
    # 4. Routerè¾“å‡º - æ¨¡æ‹Ÿtop-kè·¯ç”±ç»“æœ
    # ä¸ºæ¯ä¸ªtokenéšæœºé€‰æ‹©topkä¸ªä¸“å®¶
    topk_ids = torch.randint(0, num_experts, (M, topk), device=device, dtype=torch.int32)
    topk_weights = torch.rand(M, topk, device=device, dtype=torch.half)
    # å½’ä¸€åŒ–æƒé‡
    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
    
    print(f"   TopK IDså½¢çŠ¶: {topk_ids.shape}")
    print(f"   TopKæƒé‡å½¢çŠ¶: {topk_weights.shape}")
    
    # 5. ç”Ÿæˆlocal_topk_ids (å±•å¹³ç‰ˆæœ¬)
    local_topk_ids = topk_ids.view(-1)
    
    # 6. æ¿€æ´»é‡åŒ–scale (å¯é€‰) - å¿…é¡»æ˜¯1Då¼ é‡ [1]
    a1_scale = torch.tensor([0.01], device=device, dtype=torch.float32)
    a2_scale = torch.tensor([0.015], device=device, dtype=torch.float32)
    
    # 7. ä¸“å®¶åç§»é‡ (ç”¨äºCUTLASS grouped GEMM)
    expert_counts = torch.zeros(num_experts, dtype=torch.int32, device=device)
    for i in range(num_experts):
        expert_counts[i] = (local_topk_ids == i).sum().item()
    
    expert_offsets = torch.cumsum(torch.cat([torch.tensor([0], device=device, dtype=torch.int32), expert_counts]), dim=0).to(torch.int32)
    
    # 8. Problem sizes for grouped GEMM
    problem_sizes1 = torch.zeros(num_experts, 3, dtype=torch.int32, device=device)
    problem_sizes2 = torch.zeros(num_experts, 3, dtype=torch.int32, device=device)
    
    for i in range(num_experts):
        count = expert_counts[i].item()
        # ç¬¬ä¸€ä¸ªGEMM: [count, N*2, K]
        problem_sizes1[i] = torch.tensor([count, N * 2, K], dtype=torch.int32)
        # ç¬¬äºŒä¸ªGEMM: [count, K, N] 
        problem_sizes2[i] = torch.tensor([count, K, N], dtype=torch.int32)
    
    # 9. Strides for grouped GEMM
    a_strides1 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    b_strides1 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    c_strides1 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    
    a_strides2 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    b_strides2 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)  
    c_strides2 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    
    # è®¾ç½®strides
    for i in range(num_experts):
        # ç¬¬ä¸€ä¸ªGEMMçš„strides
        a_strides1[i] = torch.tensor([K, 1, K], dtype=torch.int64)
        b_strides1[i] = torch.tensor([K//2, 1, K//2 * N * 2], dtype=torch.int64)
        c_strides1[i] = torch.tensor([N * 2, 1, N * 2], dtype=torch.int64)
        
        # ç¬¬äºŒä¸ªGEMMçš„strides  
        a_strides2[i] = torch.tensor([N, 1, N], dtype=torch.int64)
        b_strides2[i] = torch.tensor([N//2, 1, N//2 * K], dtype=torch.int64)
        c_strides2[i] = torch.tensor([K, 1, K], dtype=torch.int64)
    
    # 10. Scale strides
    s_strides13 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    s_strides2 = torch.zeros(num_experts, 3, dtype=torch.int64, device=device)
    
    for i in range(num_experts):
        s_strides13[i] = torch.tensor([1, N * 8, 1], dtype=torch.int64)
        s_strides2[i] = torch.tensor([1, K * 4, 1], dtype=torch.int64)
    
    print(f"   ä¸“å®¶åç§»é‡: {expert_offsets}")
    print(f"   ä¸“å®¶tokenè®¡æ•°: {expert_counts}")
    
    return {
        # åŸºç¡€æ•°æ®
        'a': a,
        'w1_q': w1_q,
        'w2_q': w2_q, 
        'w1_scale': w1_scale,
        'w2_scale': w2_scale,
        'topk_weights': topk_weights,
        'topk_ids': topk_ids,
        'local_topk_ids': local_topk_ids,
        'a1_scale': a1_scale,
        'a2_scale': a2_scale,
        
        # CUTLASSæ•°æ®
        'expert_offsets': expert_offsets,
        'problem_sizes1': problem_sizes1,
        'problem_sizes2': problem_sizes2,
        'a_strides1': a_strides1,
        'b_strides1': b_strides1,
        'c_strides1': c_strides1,
        'a_strides2': a_strides2,
        'b_strides2': b_strides2,
        'c_strides2': c_strides2,
        's_strides13': s_strides13,
        's_strides2': s_strides2,
        
        # å…ƒæ•°æ®
        'M': M,
        'K': K,
        'N': N,
        'num_experts': num_experts,
        'topk': topk,
        'device': device
    }

def test_pre_reorder_kernel(data: dict):
    """æµ‹è¯•pre_reorder_triton_kernel_for_cutlass_moeç®—å­ç²¾åº¦"""
    print("\nğŸ” æµ‹è¯• pre_reorder_triton_kernel_for_cutlass_moe")
    
    try:
        from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel_for_cutlass_moe
        from sglang.srt.layers.moe.cutlass_w4a8_moe import run_cutlass_moe_ep_preproess
        
        M, K = data['M'], data['K']
        topk = data['topk']
        
        # è¿è¡Œé¢„å¤„ç†è·å–src2dstæ˜ å°„
        _, src2dst, _ = run_cutlass_moe_ep_preproess(
            data['local_topk_ids'],
            data['num_experts'],
        )
        
        # åˆ›å»ºè¾“å‡ºç¼“å†²åŒº
        gateup_input_int8 = torch.empty((M * topk, K), 
                                  device=data['device'], dtype=torch.int8)
        gateup_input_fp8 = torch.empty((M * topk, K), 
                                  device=data['device'], dtype=torch.int8)

        
        # è¿è¡Œkernel
        pre_reorder_triton_kernel_for_cutlass_moe[(M,)](
            data['a'],
            gateup_input_int8,
            src2dst,
            data['local_topk_ids'],
            data['a1_scale'],
            data['num_experts'],
            topk,
            K,
            BLOCK_SIZE=512,
        )
        pre_reorder_triton_kernel_for_cutlass_moe[(M,)](
            data['a'],
            gateup_input_fp8,
            src2dst,
            data['local_topk_ids'],
            data['a1_scale'],
            data['num_experts'],
            topk,
            K,
            BLOCK_SIZE=512,
        )
        
        print(f"   âœ… é‡æ’åºå®Œæˆ")
        print(f"   è¾“å…¥å½¢çŠ¶: {data['a'].shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {gateup_input_int8.shape}")
        print(f"   è¾“å‡ºæ•°æ®ç±»å‹: {gateup_input_int8.dtype}")
        print(f"   è¾“å‡ºèŒƒå›´: [{gateup_input_int8.min().item()}, {gateup_input_int8.max().item()}]")
        # ç®€å•ç²¾åº¦æ£€æŸ¥ - éªŒè¯é‡åŒ–æ˜¯å¦åˆç†
        expected_range = 127 * data['a1_scale'].item()
        input_range = data['a'].abs().max().item()
        print(f"   è¾“å…¥æ•°æ®èŒƒå›´: {input_range:.4f}")
        print(f"   é¢„æœŸé‡åŒ–èŒƒå›´: Â±{expected_range:.4f}")
        
        # éªŒè¯é‡åŒ–æ•°æ®çš„åˆç†æ€§
        dequant_range = gateup_input_int8.float().abs().max().item() * data['a1_scale'].item()
        print(f"   åé‡åŒ–æ•°æ®èŒƒå›´: {dequant_range:.4f}")
        quantization_error = abs(input_range - dequant_range) / input_range * 100
        print(f"   é‡åŒ–è¯¯å·®: {quantization_error:.2f}%")

        #éªŒè¯fp8å’Œint8çš„å·®è·
        diff = torch.mean(torch.abs(gateup_input_int8.float()*data['a1_scale'] - gateup_input_fp8.float()*data['a1_scale']))
        print("diff between int 8 and fp8: ", diff)
        torch.testing.assert_close(gateup_input_int8.float(), gateup_input_fp8.float(), rtol=0.01, atol=0.1)
        return gateup_input_int8, src2dst
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_cutlass_mm_data_generation(data: dict):
    """æµ‹è¯•get_cutlass_w4a8_moe_mm_dataç®—å­"""
    print("\nğŸ” æµ‹è¯• get_cutlass_w4a8_moe_mm_data")
    
    try:
        from sglang.srt.layers.moe.cutlass_w4a8_moe import get_cutlass_w4a8_moe_mm_data
        
        M, K, N = data['M'], data['K'], data['N']
        num_experts = data['num_experts']
        
        # åˆ›å»ºè¾“å‡ºæ˜ å°„
        a_map = torch.empty((data['local_topk_ids'].numel()), 
                           dtype=torch.int32, device=data['device'])
        c_map = torch.empty((data['local_topk_ids'].numel()), 
                           dtype=torch.int32, device=data['device'])
        
        # è¿è¡Œæ•°æ®ç”Ÿæˆ
        get_cutlass_w4a8_moe_mm_data(
            data['local_topk_ids'],
            data['expert_offsets'],
            data['problem_sizes1'],
            data['problem_sizes2'],
            a_map,
            c_map,
            num_experts,
            N,
            K,
        )
        
        print(f"   âœ… CUTLASSæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"   Aæ˜ å°„å½¢çŠ¶: {a_map.shape}")
        print(f"   Cæ˜ å°„å½¢çŠ¶: {c_map.shape}")
        print(f"   Aæ˜ å°„èŒƒå›´: [{a_map.min().item()}, {a_map.max().item()}]")
        print(f"   Cæ˜ å°„èŒƒå›´: [{c_map.min().item()}, {c_map.max().item()}]")
        
        return a_map, c_map
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_per_tensor_quantization(data: dict):
    """æµ‹è¯•per_tensor_quant_int8ç®—å­ç²¾åº¦"""
    print("\nğŸ” æµ‹è¯• per_tensor_quant_int8")
    
    try:
        from sglang.srt.layers.quantization.int8_kernel import per_tensor_quant_int8
        
        # åˆ›å»ºä¸­é—´ç»“æœï¼ˆæ¨¡æ‹Ÿç¬¬ä¸€æ¬¡GEMM+æ¿€æ´»åçš„ç»“æœï¼‰
        M, N = data['M'], data['N']
        topk = data['topk']
        
        intermediate = torch.randn(M * topk, N, device=data['device'], dtype=torch.half)
        
        print(f"   ä¸­é—´ç»“æœå½¢çŠ¶: {intermediate.shape}")
        print(f"   ä¸­é—´ç»“æœèŒƒå›´: [{intermediate.min().item():.4f}, {intermediate.max().item():.4f}]")
        
        # è¿è¡Œé‡åŒ–
        intermediate_q, scale = per_tensor_quant_int8(intermediate)
        
        print(f"   âœ… é‡åŒ–å®Œæˆ")
        print(f"   é‡åŒ–ç»“æœå½¢çŠ¶: {intermediate_q.shape}")
        print(f"   é‡åŒ–ç»“æœç±»å‹: {intermediate_q.dtype}")
        print(f"   é‡åŒ–èŒƒå›´: [{intermediate_q.min().item()}, {intermediate_q.max().item()}]")
        print(f"   é‡åŒ–scale: {scale.item():.8f}")
        
        # ç²¾åº¦éªŒè¯
        intermediate_dequant = intermediate_q.float() * scale
        mse = torch.mean((intermediate.float() - intermediate_dequant) ** 2)
        mae = torch.mean(torch.abs(intermediate.float() - intermediate_dequant))
        relative_error = mae / torch.mean(torch.abs(intermediate.float()))
        
        print(f"   MSE: {mse.item():.6f}")
        print(f"   MAE: {mae.item():.6f}")
        print(f"   ç›¸å¯¹è¯¯å·®: {relative_error.item():.4%}")
        
        return intermediate_q, scale
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def run_precision_tests():
    """è¿è¡Œæ‰€æœ‰ç²¾åº¦æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹MOEç®—å­ç²¾åº¦æµ‹è¯•")
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data = create_moe_test_data(
        batch_size=32,
        seq_len=128, 
        hidden_size=4096,
        intermediate_size=11008,
        num_experts=8,
        topk=2
    )
    
    # éªŒè¯å…³é”®æ•°æ®çš„å½¢çŠ¶
    # print(f"\nğŸ”§ å…³é”®å‚æ•°éªŒè¯:")
    # print(f"   a1_scaleå½¢çŠ¶: {data['a1_scale'].shape}")
    # print(f"   a2_scaleå½¢çŠ¶: {data['a2_scale'].shape}")
    # print(f"   expert_offsetså½¢çŠ¶: {data['expert_offsets'].shape}")
    # print(f"   problem_sizes1å½¢çŠ¶: {data['problem_sizes1'].shape}")
    
    # print(f"\nğŸ“Š æµ‹è¯•æ•°æ®ç»Ÿè®¡:")
    # print(f"   æ€»tokenæ•°: {data['M']}")
    # print(f"   æ€»ä¸“å®¶è°ƒç”¨æ•°: {data['M'] * data['topk']}")
    
    # æµ‹è¯•å„ä¸ªç®—å­
    results = {}
    
    # 1. æµ‹è¯•é¢„é‡æ’åº
    gateup_input, src2dst = test_pre_reorder_kernel(data)
    results['pre_reorder'] = (gateup_input, src2dst)
    
    # # 2. æµ‹è¯•CUTLASSæ•°æ®ç”Ÿæˆ
    # a_map, c_map = test_cutlass_mm_data_generation(data)
    # results['cutlass_data'] = (a_map, c_map)
    
    # # 3. æµ‹è¯•é‡åŒ–ç®—å­
    # intermediate_q, scale = test_per_tensor_quantization(data)
    # results['quantization'] = (intermediate_q, scale)
    
    print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
    success_count = sum(1 for v in results.values() if v is not None and v[0] is not None)
    total_count = len(results)
    print(f"   æˆåŠŸ: {success_count}/{total_count}")
    
    return results, data

if __name__ == "__main__":
    results, data = run_precision_tests() 